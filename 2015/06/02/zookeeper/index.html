<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>zookeeper | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="通过一篇文章，讲清楚关于zookeeper的内容">
<meta property="og:type" content="article">
<meta property="og:title" content="zookeeper">
<meta property="og:url" content="http://thinkhejie.github.io/2015/06/02/zookeeper/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="通过一篇文章，讲清楚关于zookeeper的内容">
<meta property="og:updated_time" content="2017-06-22T14:52:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="zookeeper">
<meta name="twitter:description" content="通过一篇文章，讲清楚关于zookeeper的内容">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/img/author.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">thinkhejie</a></h1>
		</hgroup>

		
		<p class="header-subtitle">热爱大海与冷笑话</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/categories/Java">Java</a></li>
				        
							<li><a href="/categories/dubbo">Dubbo</a></li>
				        
							<li><a href="/categories/Git">Git</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/thinkhejie" title="github">github</a>
					        
								<a class="mail" target="_blank" href="/thinkhejie@gmail.com" title="mail">mail</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Cgroup/" style="font-size: 10px;">Cgroup</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Guava/" style="font-size: 10px;">Guava</a> <a href="/tags/Hbase/" style="font-size: 16px;">Hbase</a> <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Java/" style="font-size: 18px;">Java</a> <a href="/tags/LXC/" style="font-size: 10px;">LXC</a> <a href="/tags/Linux/" style="font-size: 14px;">Linux</a> <a href="/tags/NIO/" style="font-size: 12px;">NIO</a> <a href="/tags/SQL/" style="font-size: 10px;">SQL</a> <a href="/tags/docker/" style="font-size: 12px;">docker</a> <a href="/tags/dubbo/" style="font-size: 20px;">dubbo</a> <a href="/tags/execption/" style="font-size: 10px;">execption</a> <a href="/tags/java/" style="font-size: 10px;">java</a> <a href="/tags/jvm/" style="font-size: 14px;">jvm</a> <a href="/tags/kill/" style="font-size: 12px;">kill</a> <a href="/tags/maven/" style="font-size: 10px;">maven</a> <a href="/tags/mmap/" style="font-size: 10px;">mmap</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/navicat/" style="font-size: 10px;">navicat</a> <a href="/tags/netty/" style="font-size: 10px;">netty</a> <a href="/tags/sar/" style="font-size: 10px;">sar</a> <a href="/tags/sql/" style="font-size: 10px;">sql</a> <a href="/tags/ulimit/" style="font-size: 10px;">ulimit</a> <a href="/tags/zab/" style="font-size: 10px;">zab</a> <a href="/tags/zookeeper/" style="font-size: 10px;">zookeeper</a> <a href="/tags/令牌桶/" style="font-size: 10px;">令牌桶</a> <a href="/tags/分布式/" style="font-size: 10px;">分布式</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">thinkhejie</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="/img/author.png" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">thinkhejie</h1>
			</hgroup>
			
			<p class="header-subtitle">热爱大海与冷笑话</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories/Java">Java</a></li>
		        
					<li><a href="/categories/dubbo">Dubbo</a></li>
		        
					<li><a href="/categories/Git">Git</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/thinkhejie" title="github">github</a>
			        
						<a class="mail" target="_blank" href="/thinkhejie@gmail.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap"><article id="post-zookeeper" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/06/02/zookeeper/" class="article-date">
  	<time datetime="2015-06-02T13:04:38.000Z" itemprop="datePublished">2015-06-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      zookeeper
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/docker/">docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/zab/">zab</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式/">分布式</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/zookeeper/">zookeeper</a>
	</div>


        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>通过一篇文章，讲清楚关于zookeeper的内容</p>
<a id="more"></a>
<h2 id="zookeeper-使用场景"><a href="#zookeeper-使用场景" class="headerlink" title="zookeeper 使用场景"></a>zookeeper 使用场景</h2><p>1、集群配置数据管理<br>2、配置中心(soa)<br>3、简单消息推送<br>4、分布式锁</p>
<h2 id="zookeeper-zab协议"><a href="#zookeeper-zab协议" class="headerlink" title="zookeeper zab协议"></a>zookeeper zab协议</h2><p>ZAB：ZooKeeper的Atomic Broadcast协议，能够保证发给各副本的消息顺序相同。<br>Zookeeper使用了一种称为Zab（ZookeeperAtomic Broadcast）的协议作为其一致性复制的核心，其特点为高吞吐量、低延迟、健壮、简单，但不过分要求其扩展性。</p>
<p>Zookeeper的实现是有Client、Server构成，Server端提供了一个一致性复制、存储服务，Client端会提供一些具体的语义，比如分布式锁、选举算法、分布式互斥等。从存储内容来说，Server端更多的是存储一些数据的状态，而非数据内容本身，因此Zookeeper可以作为一个小文件系统使用。数据状态的存储量相对不大，完全可以全部加载到内存中，从而极大地消除了通信延迟。<br>Server可以Crash后重启，考虑到容错性，Server必须“记住”之前的数据状态，因此数据需要持久化，但吞吐量很高时，磁盘的IO便成为系统瓶颈，其解决办法是使用缓存，把随机写变为连续写。</p>
<p>考虑到Zookeeper主要操作数据的状态，为了保证状态的一致性，Zookeeper提出了两个安全属性（Safety Property）：<br>全序（Total order）：如果消息a在消息b之前发送，则所有Server应该看到相同的结果<br>因果顺序（Causal order）：如果消息a在消息b之前发生（a导致了b），并被一起发送，则a始终在b之前被执行。<br>为了保证上述两个安全属性，Zookeeper使用了TCP协议和Leader。通过使用TCP协议保证了消息的全序特性（先发先到），通过Leader解决了因果顺序问题：先到Leader的先执行。因为有了Leader，Zookeeper的架构就变为：Master-Slave模式，但在该模式中Master（Leader）会Crash，因此，Zookeeper引入了Leader选举算法，以保证系统的健壮性。归纳起来Zookeeper整个工作分两个阶段：<br>Atomic Broadcast<br>Leader选举<br>1.Atomic Broadcast<br>同一时刻存在一个Leader节点，其他节点称为“Follower”，如果是更新请求，如果客户端连接到Leader节点，则由Leader节点执行其请求；如果连接到Follower节点，则需转发请求到Leader节点执行。但对读请求，Client可以直接从Follower上读取数据，如果需要读到最新数据，则需要从Leader节点进行，Zookeeper设计的读写比例是2：1。</p>
<p>Leader通过一个简化版的二段提交模式向其他Follower发送请求，但与二段提交有两个明显的不同之处：<br>• 因为只有一个Leader，Leader提交到Follower的请求一定会被接受（没有其他Leader干扰）<br>• 不需要所有的Follower都响应成功，只要一个多数派即可<br>通俗地说，如果有2f+1个节点，允许f个节点失败。因为任何两个多数派必有一个交集，当Leader切换时，通过这些交集节点可以获得当前系统的最新状态。如果没有一个多数派存在（存活节点数小于f+1）则，算法过程结束。但有一个特例：<br>如果有A、B、C三个节点，A是Leader，如果B Crash，则A、C能正常工作，因为A是Leader，A、C还构成多数派；如果A Crash则无法继续工作，因为Leader选举的多数派无法构成。</p>
<p>2.Leader Election<br>Leader选举主要是依赖Paxos算法，Leader选举遇到的最大问题是，”新老交互“的问题，新Leader是否要继续老Leader的状态。这里要按老Leader Crash的时机点分几种情况：<br>1.老Leader在COMMIT前Crash（已经提交到本地）<br>2.老Leader在COMMIT后Crash，但有部分Follower接收到了Commit请求<br>第一种情况，这些数据只有老Leader自己知道，当老Leader重启后，需要与新Leader同步并把这些数据从本地删除，以维持状态一致。<br>第二种情况，新Leader应该能通过一个多数派获得老Leader提交的最新数据，老Leader重启后，可能还会认为自己是Leader，可能会继续发送未完成的请求，从而因为两个Leader同时存在导致算法过程失败，解决办法是把Leader信息加入每条消息的id中，Zookeeper中称为zxid，zxid为一64位数字，高32位为leader信息又称为epoch，每次leader转换时递增；低32位为消息编号，Leader转换时应该从0重新开始编号。通过zxid，Follower能很容易发现请求是否来自老Leader，从而拒绝老Leader的请求。<br>因为在老Leader中存在着数据删除（情况1），因此Zookeeper的数据存储要支持补偿操作，这也就需要像数据库一样记录log。</p>
<p>3.Zab与Paxos<br>Zab的作者认为Zab与paxos并不相同，Zab就是Paxos的一种简化形式，Paxos保证不了全序顺序。<br>这里首要一点是Paxos的一致性不能达到ZooKeeper的要求。举个例子：假设一开始Paxos系统中的leader是P1，他发起了两个事务<t1, v1="">（表示序号为t1的事务要写的值是v1）和<t2, v2="">，过程中挂了。新来个leader是P2，他发起了事务<t1, v1'="">。而后又来个新leader是P3，他汇总了一下，得出最终的执行序列<t1, v1'="">和<t2, v2="">，即P2的t1在前，P1的t2在后。<br>分析为什么不满足ZooKeeper需求：<br>ZooKeeper是一个树形结构，很多操作都要先检查才能确定能不能执行，比如P1的事务t1可能是创建节点“/a”，t2可能是创建节点“/a/aa”，只有先创建了父节点“/a”，才能创建子节点“/a/aa”。而P2所发起的事务t1可能变成了创建“/b”。这样P3汇总后的序列是先创建“/b”再创建“/a/aa”，由于“/a”还没建，创建“a/aa”就搞不定了。<br>解决方案：<br>为了保证这一点，ZAB要保证同一个leader的发起的事务要按顺序被apply，同时还要保证只有先前的leader的所有事务都被apply之后，新选的leader才能在发起事务。</t2,></t1,></t1,></t2,></t1,></p>
<p>ZAB的核心思想：形象的说就是保证任意时刻只有一个节点是leader，所有更新事务由leader发起去更新所有复本（称为follower），更新时用的就是两阶段提交协议，只要多数节点prepare成功，就通知他们commit。各follower要按当初leader让他们prepare的顺序来apply事务。因为ZAB处理的事务永远不会回滚，ZAB的2PC做了点优化，多个事务只要通知zxid最大的那个commit，之前的各follower会统统commit。<br>这里有几个关键点：<br>1、leader所follower之间通过心跳来检测异常；<br>2、检测到异常之后的节点若试图成为新的leader，首先要获得大多数节点的支持，然后从状态最新的节点同步事务，完成后才可正式成为leader发起事务；<br>3、区分新老leader的关键是一个会一直增长的epoch；</p>
<h2 id="zookeeper-选举流程"><a href="#zookeeper-选举流程" class="headerlink" title="zookeeper 选举流程"></a>zookeeper 选举流程</h2><table>
<thead>
<tr>
<th>角色</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Leader(领导者)</td>
<td>为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。</td>
</tr>
<tr>
<td>Follower（跟随者）</td>
<td>为客户端提供读服务，如果是写服务则转发给Leader。在选举过程中参与投票。</td>
</tr>
<tr>
<td>Observe（观察者）</td>
<td>为客户端提供读服务器，如果是写服务则转发给Leader。不参与选举过程中的投票，也不参与“过半写成功”策略。在不影响写性能的情况下提升集群的读性能。</td>
</tr>
<tr>
<td>client（客户端）</td>
<td>连接zookeeper服务器的使用着，请求的发起者。独立于zookeeper服务器集群之外的角色。</td>
</tr>
</tbody>
</table>
<h2 id="zookeeper-ACL"><a href="#zookeeper-ACL" class="headerlink" title="zookeeper ACL"></a>zookeeper ACL</h2><h2 id="zookeeper-客户端"><a href="#zookeeper-客户端" class="headerlink" title="zookeeper 客户端"></a>zookeeper 客户端</h2><p>目前Curator有2.x.x和3.x.x两个系列的版本，支持不同版本的Zookeeper。其中Curator 2.x.x兼容Zookeeper的3.4.x和3.5.x。Curator 3.x.x只兼容Zookeeper 3.5.x，并且提供了一些诸如动态重新配置、watch删除等新特性。</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Recipes</td>
<td>Zookeeper典型应用场景的实现，这些实现是基于Curator Framework。</td>
</tr>
<tr>
<td>Framework</td>
<td>Zookeeper API的高层封装，大大简化Zookeeper客户端编程，添加了例如Zookeeper连接管理、重试机制等。</td>
</tr>
<tr>
<td>Utilities</td>
<td>为Zookeeper提供的各种实用程序。</td>
</tr>
<tr>
<td>Client</td>
<td>Zookeeper client的封装，用于取代原生的Zookeeper客户端（ZooKeeper类），提供一些非常有用的客户端特性。</td>
</tr>
<tr>
<td>Errors</td>
<td>Curator如何处理错误，连接问题，可恢复的例外等。</td>
</tr>
</tbody>
</table>
<h2 id="zookeeper-文件与快照"><a href="#zookeeper-文件与快照" class="headerlink" title="zookeeper 文件与快照"></a>zookeeper 文件与快照</h2><p>日志路径:/datalog/version-2<br>快照路径:/data/version-2</p>
<p>1、当开始第一次运行的时候，首先会产生一个snapshot，这个时候是没有log的。当第一次有操作的时候开始log文件产生的时候，这时候不会产生snapshot。<br>当第一个log文件体积达到预设值时候，这时候系统需要snapshot，然后创建新的日志文件，以此不断交替。<br>2、生成snapshot触发条件: logcount &gt; (snapcount/2 + randroll)  randroll是随机数范围在[1, snapcount/2]</p>
<h2 id="zookeeper-网络相关"><a href="#zookeeper-网络相关" class="headerlink" title="zookeeper 网络相关"></a>zookeeper 网络相关</h2><p>1、zookeeper的网络请求与连接通过netty实现</p>
<h2 id="zookeeper-session管理"><a href="#zookeeper-session管理" class="headerlink" title="zookeeper session管理"></a>zookeeper session管理</h2><h2 id="zookeeper-运维命令"><a href="#zookeeper-运维命令" class="headerlink" title="zookeeper 运维命令"></a>zookeeper 运维命令</h2><p>1、zookeeper的命令分为两部分，一块是客户端命令，一块是服务端命令。通过所说的四字命令，指的是服务端的命令。</p>
<p>2、客户端命令</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>stat</td>
<td>stat path [watch]</td>
</tr>
<tr>
<td>2</td>
<td>set</td>
<td>set path data [version]</td>
</tr>
<tr>
<td>3</td>
<td>ls</td>
<td>ls path [watch]</td>
</tr>
<tr>
<td>4</td>
<td>delquota</td>
<td>delquota [-n</td>
<td>-b] path</td>
</tr>
<tr>
<td>5</td>
<td>ls2</td>
<td>ls2 path [watch]</td>
</tr>
<tr>
<td>6</td>
<td>setAcl</td>
<td>setAcl path acl</td>
</tr>
<tr>
<td>7</td>
<td>setquota</td>
<td>setquota -n</td>
<td>-b val path</td>
</tr>
<tr>
<td>8</td>
<td>history</td>
<td>history</td>
</tr>
<tr>
<td>9</td>
<td>redo</td>
<td>redo cmdno</td>
</tr>
<tr>
<td>10</td>
<td>printwatches</td>
<td>printwatches on</td>
<td>off</td>
</tr>
<tr>
<td>11</td>
<td>delete</td>
<td>delete path [version]</td>
</tr>
<tr>
<td>12</td>
<td>sync</td>
<td>sync path</td>
</tr>
<tr>
<td>13</td>
<td>listquota</td>
<td>listquota path</td>
</tr>
<tr>
<td>14</td>
<td>rmr</td>
<td>rmr path</td>
</tr>
<tr>
<td>15</td>
<td>get</td>
<td>get path [watch]</td>
</tr>
<tr>
<td>16</td>
<td>create</td>
<td>create [-s] [-e] path data acl</td>
</tr>
<tr>
<td>17</td>
<td>addauth</td>
<td>addauth scheme auth</td>
</tr>
<tr>
<td>18</td>
<td>quit</td>
<td>quit</td>
</tr>
<tr>
<td>19</td>
<td>getAcl</td>
<td>getAcl path</td>
</tr>
<tr>
<td>20</td>
<td>close</td>
<td>close</td>
</tr>
<tr>
<td>21</td>
<td>connect</td>
<td>connect host:port</td>
</tr>
</tbody>
</table>
<p>3、服务端命令</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>命令</th>
<th>功能描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>conf</td>
<td>输出相关服务配置的详细信息。</td>
</tr>
<tr>
<td>2</td>
<td>cons</td>
<td>列出所有连接到服务器的客户端的完全的连接/会话的详细信息。包括“接受 / 发送”的包数量、会话 id 、操作延迟、最后的操作执行等等信息。</td>
</tr>
<tr>
<td>3</td>
<td>dump</td>
<td>列出未经处理的会话和临时节点。</td>
</tr>
<tr>
<td>4</td>
<td>envi</td>
<td>输出关于服务环境的详细信息（区别于 conf 命令）。</td>
</tr>
<tr>
<td>5</td>
<td>reqs</td>
<td>列出未经处理的请求</td>
</tr>
<tr>
<td>6</td>
<td>ruok</td>
<td>测试服务是否处于正确状态。如果确实如此，那么服务返回“ imok ”，否则不做任何相应。</td>
</tr>
<tr>
<td>7</td>
<td>stat</td>
<td>输出关于性能和连接的客户端的列表。</td>
</tr>
<tr>
<td>8</td>
<td>wchs</td>
<td>列出服务器 watch 的详细信息。</td>
</tr>
<tr>
<td>9</td>
<td>wchc</td>
<td>通过 session 列出服务器 watch 的详细信息，它的输出是一个与 watch 相关的会话的列表。</td>
</tr>
<tr>
<td>10</td>
<td>wchp</td>
<td>通过路径列出服务器 watch 的详细信息。它输出一个与 session 相关的路径。</td>
</tr>
</tbody>
</table>
<p>4、用法举例</p>
<p>echo stat | nc 127.0.0.1 2181 查看哪个节点被选择作为follower或者leader<br>echo ruok | nc 127.0.0.1 2181 测试是否启动了该Server，若回复imok表示已经启动。<br>echo dump | nc 127.0.0.1 2181 列出未经处理的会话和临时节点。<br>echo kill | nc 127.0.0.1 2181 关掉server<br>echo conf | nc 127.0.0.1 2181 输出相关服务配置的详细信息。<br>echo cons | nc 127.0.0.1 2181 列出所有连接到服务器的客户端的完全的连接 / 会话的详细信息。<br>echo envi | nc 127.0.0.1 2181 输出关于服务环境的详细信息（区别于 conf 命令）。<br>echo reqs | nc 127.0.0.1 2181 列出未经处理的请求。<br>echo wchs | nc 127.0.0.1 2181 列出服务器watch的详细信息。<br>echo wchc | nc 127.0.0.1 2181 通过session列出服务器watch的详细信息，它的输出是一个与 watch 相关的会话的列表。<br>echo wchp | nc 127.0.0.1 2181 通过路径列出服务器watch的详细信息。它输出一个与 session 相关的路径。</p>
<p>5、启停命令<br>./zkServer.sh restart<br>./zkServer.sh start<br>./zkServer.sh stop</p>
<p>6、启用全部四字命令</p>
<p><1>、在conf目录下创建文件java.env(如果不存在的话)</1></p>
<p><2>、添加语句: export JVMFLAGS=”-Dzookeeper.4lw.commands.whitelist=* $JVMFLAGS”</2></p>
<h2 id="zookeeper-实现分布式锁"><a href="#zookeeper-实现分布式锁" class="headerlink" title="zookeeper 实现分布式锁"></a>zookeeper 实现分布式锁</h2><p>大致思想即为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。</p>
<p>锁无法释放？使用Zookeeper可以有效的解决锁无法释放的问题，因为在创建锁的时候，客户端会在ZK中创建一个临时节点，一旦客户端获取到锁之后突然挂掉（Session连接断开），那么这个临时节点就会自动删除掉。其他客户端就可以再次获得锁。<br>非阻塞锁？使用Zookeeper可以实现阻塞的锁，客户端可以通过在ZK中创建顺序节点，并且在节点上绑定监听器，一旦节点有变化，Zookeeper会通知客户端，客户端可以检查自己创建的节点是不是当前所有节点中序号最小的，如果是，那么自己就获取到锁，便可以执行业务逻辑了。<br>不可重入？使用Zookeeper也可以有效的解决不可重入的问题，客户端在创建节点的时候，把当前客户端的主机信息和线程信息直接写入到节点中，下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。如果和自己的信息一样，那么自己直接获取到锁，如果不一样就再创建一个临时的顺序节点，参与排队。<br>单点问题？使用Zookeeper可以有效的解决单点问题，ZK是集群部署的，只要集群中有半数以上的机器存活，就可以对外提供服务。</p>
<p>使用ZK实现的分布式锁好像完全符合了本文开头我们对一个分布式锁的所有期望。但是，其实并不是，Zookeeper实现的分布式锁其实存在一个缺点，那就是性能上可能并没有缓存服务那么高。因为每次在创建锁和释放锁的过程中，都要动态创建、销毁瞬时节点来实现锁功能。ZK中创建和删除节点只能通过Leader服务器来执行，然后将数据同步到所有的Follower机器上。</p>
<p>其实，使用Zookeeper也有可能带来并发问题，只是并不常见而已。考虑这样的情况，由于网络抖动，客户端可ZK集群的session连接断了，那么zk以为客户端挂了，就会删除临时节点，这时候其他客户端就可以获取到分布式锁了，就可能产生并发问题。这个问题不常见是因为zk有重试机制，一旦zk集群检测不到客户端的心跳，就会重试，Curator客户端支持多种重试策略。多次重试之后还不行的话才会删除临时节点。（所以，选择一个合适的重试策略也比较重要，要在锁的粒度和并发之间找一个平衡。）</p>
<p>使用Zookeeper实现分布式锁的优点</p>
<p>1、有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。</p>
<p>使用Zookeeper实现分布式锁的缺点</p>
<p>2、性能上不如使用缓存实现分布式锁。 需要对ZK的原理有所了解。</p>
<p>分布式写锁<br>1.create一个PERSISTENT类型的znode，/Locks/write_lock<br>2.客户端创建SEQUENCE|EPHEMERAL类型的znode，名字是lockid开头，创建的znode是/Locks/write_lock/lockid0000000001<br>3.调用getChildren()不要设置Watcher获取/Locks/write_lock下的znode列表<br>4.判断自己步骤2创建znode是不是znode列表中最小的一个，如果是就代表获得了锁，如果不是往下走,调用exists()判断步骤2自己创建的节点编号小1的znode节点(也就是获取的znode节点列表中最小的znode)，并且设置Watcher，如果exists()返回false，执行步骤3,如果exists()返回true,那么等待zk通知，从而在回掉函数里返回执行步骤3</p>
<h2 id="通过docker-在mac安装单机zookeeper"><a href="#通过docker-在mac安装单机zookeeper" class="headerlink" title="通过docker 在mac安装单机zookeeper"></a>通过docker 在mac安装单机zookeeper</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host -p 2181:2181,2888:2888,3888:3888 --name zoo00 -d zookeeper:latest</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 一般语法</span><br><span class="line">./zkCli.sh -timeout 0 -r -server ip:port</span><br><span class="line"></span><br><span class="line"># 连接到主机h1 超时时间3秒</span><br><span class="line">./zkCli.sh -timeout 3000 -server h1:2181</span><br><span class="line"></span><br><span class="line"># 有类似如下的命令提示符就表示连接成功了</span><br><span class="line">[zk: h1:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">##docker进入客户端命令行模式</span><br><span class="line">docker run -it --name my_zookeeper_1:zookeeper zookeeper zkCli.sh -server zookeeper</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##进入zk内部</span></span><br><span class="line"><span class="comment">##需要exec进入Docker容器配置myid和hosts文件</span></span><br><span class="line">docker <span class="built_in">exec</span> -it <span class="variable">$&#123;容器id&#125;</span> /bin/bash</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker port <span class="variable">$&#123;容器id&#125;</span> 5000</span><br><span class="line">$ 127.0.0.1:49155</span><br></pre></td></tr></table></figure>
<h2 id="通过docker-在mac安装集群zookeeper"><a href="#通过docker-在mac安装集群zookeeper" class="headerlink" title="通过docker 在mac安装集群zookeeper"></a>通过docker 在mac安装集群zookeeper</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 首次执行，创建一个zk集群</span></span><br><span class="line">COMPOSE_PROJECT_NAME=zk_<span class="built_in">test</span> docker-compose up <span class="_">-d</span></span><br><span class="line"><span class="comment">##  以后运行，可以通过start\stop 控制启停</span></span><br><span class="line">COMPOSE_PROJECT_NAME=zk_<span class="built_in">test</span> docker-compose start</span><br><span class="line">COMPOSE_PROJECT_NAME=zk_<span class="built_in">test</span> docker-compose stop</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ COMPOSE_PROJECT_NAME=zk_<span class="built_in">test</span> docker-compose ps</span><br><span class="line">Name              Command               State           Ports</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">zoo1   /docker-entrypoint.sh zkSe ...   Up      0.0.0.0:2181-&gt;2181/tcp</span><br><span class="line">zoo2   /docker-entrypoint.sh zkSe ...   Up      0.0.0.0:2182-&gt;2181/tcp</span><br><span class="line">zoo3   /docker-entrypoint.sh zkSe ...   Up      0.0.0.0:2183-&gt;2181/tcp</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --rm \</span><br><span class="line">        --link zoo1:zk1 \</span><br><span class="line">        --link zoo2:zk2 \</span><br><span class="line">        --link zoo3:zk3 \</span><br><span class="line">        --net zktest_default \</span><br><span class="line">        zookeeper zkCli.sh -server zk1:2181,zk2:2181,zk3:2181</span><br></pre></td></tr></table></figure>
<p>通过本地主机连接ZK集群<br>因为我们分别将 zoo1, zoo2, zoo3 的 2181 端口映射到了 本地主机的2181, 2182, 2183 端口上, 因此我们使用如下命令即可连接 ZK 集群了:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkCli.sh -server localhost:2181,localhost:2182,localhost:2183</span><br></pre></td></tr></table></figure>
<h2 id="zookeeper-源码解析"><a href="#zookeeper-源码解析" class="headerlink" title="zookeeper 源码解析"></a>zookeeper 源码解析</h2><p>客户端部分<br>WatchManager<br>ClientCnxn<br>ClientCnxnSocket<br>ClientCnxnSocketNetty/ClientCnxnSocketNIO</p>
<p>服务端部分<br>NIOServerCnxn/NettyServerCnxn</p>
<p>存储部分<br>ZKDatabase</p>
<p>监控部分<br>OSMXBean</p>
<p>选举部分<br>FastLeaderElection</p>
<h2 id="zookeeper-相关问题"><a href="#zookeeper-相关问题" class="headerlink" title="zookeeper 相关问题"></a>zookeeper 相关问题</h2><p>在Docker环境下，通常虚机会比较多，我们发现ZooKeeper不能承受太多节点。我们的游戏平台是一个多租户场景，需要频繁进行创建及删除，在这种情况下如果超过3000虚拟机，ZooKeeper就不行了。<br>当ZooKeeper挂了后，meta数据也会跟着丢了。Meta数据和和监控功能在ZooKeeper当初设计时并没有考虑的问题，因此后来我们打算自己造轮子，将这个需求实现。<br>我也并非说ZooKeeper一无是处，ZooKeeper有它适合使用的场景，比如Hadoop那种场景ZooKeeper就可以工作得很好，但并不是所有场景都适用。Tim说过一句经典的话，Redis是把好锤子，但不能把所有存储的问题都当做钉子。ZooKeeper也是一样，不是所有的配置的问题都适合拿它来解决。<br>其他一些网友的看法。<br>Z: 同意ZooKeeper是有很多运维的问题，一个解决方法是自己实现一个single node all in memory的lock service，然后运行在多个机器上，用ZooKeeper当作一个distributed lock来选一个master，这样ZooKeeper上的压力就小多了，而且不会随着集群大小增加而增加。<br>Y: Kafka里使用ZooKeeper的方式有好几个地方可以借鉴。除了上面说的选一个master处理的方式外，为了避免订阅大量节点，也可以单设置一个变更节点，然后只订阅这个变更节点。<br>G: Kafka集群依赖于ZooKeeper，但ZooKeeper也是Kafka的瓶颈。</p>
<p>某知名大型互联网公司的架构师也基本认可上述观点，其新开发的服务框架中不再采用ZooKeeper作为注册中心，主要说明如下<br>1.随着部署规模增大，客户端增多，ZooKeeper服务器中节点数量大增，核心ZooKeeper集群5台，每台服务器15000左右的长链接，近43万个节点，平均30余万watch。每个上线日，ZooKeeper服务器的流量都能上200mb/s，通过mntr的观察zk_outstanding_requests经常达到300M以上;</p>
<p>2.有业务方报告，应用启动后连上ZooKeeper但一直读失败(读数据返回为null)，也就是取服务列表失败，一段时间如半小时后自动恢复，查看zk server端日志，只看到一堆session过期的警告，没有其他异常;</p>
<p>3.由于历史原因，ZooKeeper服务器与其他应用共用，40万+节点中60%是我们的，其他应用也是重度依赖zookeeper，互相之间有影响;</p>
<p>4.ZooKeeper推送的频率、内容，我们自己不能控制，比如一个600 provider，12000 consumer的服务，如果新增一个provider，其实并不是每个consumer都需要通知的，但目前的机制下consumer监听一个目录，每个consumer都会得到一次provider列表推送;还有我们想在推送前根据一定的规则对provider列表做动态过滤排序，这个需求在zookeeper服务端也没法实现。</p>
<p>5.ZooKeeper客户端使用的是 172.17.xx.yy,172.17.xx.yz:2181 这样的ip串方式连接，后期想添加服务器分散主集群读写压力，也不好实现，因为需要更改地址串，需要update配置的客户端太多了;</p>
<p>6.跨机房容灾方面，一旦出现机房间通信问题，另一个机房的部署的Observer节点，就不可读写了;</p>
<p>7.provider约6万实例，consumer22万实例;<br>基于以上的原因，新的方案实现也很简单，自己实现了一个服务用来做注册中心的，实现的是服务注册/订阅方面的功能。它负责接收长链接并推送，数据存储在MySQL。</p>
<p>Q：使用ZooKeeper主要是它实现了强一致性，你这个注册中心是单点的吧?master election用ZooKeeper?注册中心会成为新的瓶颈和故障点?<br>A：性能方面完全够用，因为这个服务也是集群部署的，客户端首先访问一个http接口拿到所有服务器的地址，并优先访问本机房的注册中心。注册中心基本读多写少，内存里面也缓存provider列表，不会有太大压力到MySQL;存储上通过MySQL实现一致性保证。</p>
<p>Q：跨机房容灾方面有什么优化吗?如果网络没有足够的redundency，一旦出现network partiton，那么有一个机房就没办法到quorum了?<br>A：通过多机房部署，每个机房保证至少2个节点，本机房挂了还可以访问其它机房，并且在客户端本地还有缓存文件。</p>
<p>Q：当配置变化，client以http轮询方式去感知吗?<br>A：Session过期那个，开始我们设的是很短，后来发现不对，网络一闪断，大量掉节点。现在我们自己实现的服务，半分钟一心跳，几个心跳没收到我才认为下线了。如果provider列表发生变化，我这边服务端会主动推送的，因为用的是TCP长连接。</p>
<p>Q: Client有没有主动发现本地cache与服务端数据不一致的机制?例如当provider列表发生变化且服务端通过TCP长连给client推送失败的场景。<br>A：有，我们心跳带本地版本号的，如果与服务端不一致的话，会触发服务端再次推送;</p>
<p>而另外一知名大公司架构师认为如果consumer数量没有那么多，用ZooKeeper也能较好满足要求。其业务场景的ZooKeepr使用及运行情况如下。<br>线上的单个ZooKeeper节点平均连接数是6K，watcher是30万，netIO不高，ZooKeeper的机器配置比较差，用的是4G 4CPU 的虚拟机，5个节点，最多的consumer不超过200，平均consumer数量就5，6个左右。<br>目前线上provider数3万左右实例数，consumer数10万左右，目前运行的没什么问题。3万个，如果每个服务10个实例，也就3,000个服务，全公司的，也没多少。看起来是多了点。 跟公司发展历史及使用场景关系很大。<br>之前关于ZooKeeper踩坑最多的是在客户端上，最开始用的是netflix的curator(后来贡献给apache，但是我们用的是老的，没升级)，遇到网络闪断重连不上，然后死循环一样，升级到apache curator就好了。<br>后来遇到一个问题是，如果注册watcher太多，发生重连的时候，zk client会自动注册之前的所有watcher，这样会导致一个包的大小超过1M，然后也就重连不上又不断地重连，后来hack了zk client解决了这个问题，这个bug到目前为止zk官方仍然没有修复。<br>不过这些坑基本上遇到一个可以解决一个，但是目前有一个问题目前也没找到好的解决方案，那就是业务方系统load变高，或者发生长时间gc，导致zk重连甚至session过期。<br>另外关于zk连接，我们在最底下做了连接共享，因为好多服务都依赖zk，这个也降低了不少连接数。</p>
<p>1.客户端对ServerList的轮询机制是什么<br> 随机，客户端在初始化( new ZooKeeper(String connectString, int sessionTimeout, Watcher watcher) )的过程中，将所有Server保存在一个List中，然后随机打散，形成一个环。之后从0号位开始一个一个使用。<br>两个注意点：1. Server地址能够重复配置，这样能够弥补客户端无法设置Server权重的缺陷，但是也会加大风险。（比如: 192.168.1.1:2181,192.168.1.1:2181,192.168.1.2:2181). 2. 如果客户端在进行Server切换过程中耗时过长，那么将会收到SESSION_EXPIRED. 这也是上面第1点中的加大风险之处。</p>
<p>2.客户端如何正确处理CONNECTIONLOSS(连接断开) 和 SESSIONEXPIRED(Session 过期)两类连接异常<br>在ZooKeeper中，服务器和客户端之间维持的是一个长连接，在 SESSION_TIMEOUT 时间内，服务器会确定客户端是否正常连接(客户端会定时向服务器发送heart_beat),服务器重置下次SESSION_TIMEOUT时间。因此，在正常情况下，Session一直有效，并且zk集群所有机器上都保存这个Session信息。在出现问题情况下，客户端与服务器之间连接断了（客户端所连接的那台zk机器挂了，或是其它原因的网络闪断），这个时候客户端会主动在地址列表（初始化的时候传入构造方法的那个参数connectString）中选择新的地址进行连接。<br>好了，上面基本就是服务器与客户端之间维持长连接的过程了。在这个过程中，用户可能会看到两类客异常CONNECTIONLOSS(连接断开) 和SESSIONEXPIRED(Session 过期)。<br>CONNECTIONLOSS发生在上面红色文字部分，应用在进行操作A时，发生了CONNECTIONLOSS，此时用户不需要关心我的会话是否可用，应用所要做的就是等待客户端帮我们自动连接上新的zk机器，一旦成功连接上新的zk机器后，确认刚刚的操作A是否执行成功了。<br>SESSIONEXPIRED发生在上面蓝色文字部分，这个通常是zk客户端与服务器的连接断了，试图连接上新的zk机器，这个过程如果耗时过长，超过 SESSION_TIMEOUT 后还没有成功连接上服务器，那么服务器认为这个session已经结束了（服务器无法确认是因为其它异常原因还是客户端主动结束会话），开始清除和这个会话有关的信息，包括这个会话创建的临时节点和注册的Watcher。在这之后，客户端重新连接上了服务器在，但是很不幸，服务器会告诉客户端SESSIONEXPIRED。此时客户端要做的事情就看应用的复杂情况了，总之，要重新实例zookeeper对象，重新操作所有临时数据（包括临时节点和注册Watcher）。</p>
<p>3.不同的客户端对同一个节点是否能获取相同的数据</p>
<p>4.一个客户端修改了某个节点的数据，其它客户端能够马上获取到这个最新数据吗<br>ZooKeeper不能确保任何客户端能够获取（即Read Request）到一样的数据，除非客户端自己要求：方法是客户端在获取数据之前调用org.apache.zookeeper.AsyncCallback.VoidCallback, java.lang.Object) sync.<br>通常情况下（这里所说的通常情况满足：1. 对获取的数据是否是最新版本不敏感，2. 一个客户端修改了数据，其它客户端需要不需要立即能够获取最新），可以不关心这点。<br>在其它情况下，最清晰的场景是这样：ZK客户端A对 /my_test 的内容从 v1-&gt;v2, 但是ZK客户端B对 /my_test 的内容获取，依然得到的是 v1. 请注意，这个是实际存在的现象，当然延时很短。解决的方法是客户端B先调用 sync(), 再调用 getData().</p>
<p>5.ZK为什么不提供一个永久性的Watcher注册机制<br>不支持用持久Watcher的原因很简单，ZK无法保证性能。</p>
<p>6.使用watch需要注意的几点<br>   a. Watches通知是一次性的，必须重复注册.<br>   b. 发生CONNECTIONLOSS之后，只要在session_timeout之内再次连接上（即不发生SESSIONEXPIRED），那么这个连接注册的watches依然在。<br>   c. 节点数据的版本变化会触发NodeDataChanged，注意，这里特意说明了是版本变化。存在这样的情况，只要成功执行了setData()方法，无论内容是否和之前一致，都会触发NodeDataChanged。<br>   d. 对某个节点注册了watch，但是节点被删除了，那么注册在这个节点上的watches都会被移除。<br>   e. 同一个zk客户端对某一个节点注册相同的watch，只会收到一次通知。即</p>
<p>for( int i = 0; i &lt; 3; i++ ){<br>    zk.getData( path, true, null );<br>    zk.getChildren( path, true );<br>}</p>
<p>7.我能否收到每次节点变化的通知<br> 如果节点数据的更新频率很高的话，不能。<br> 原因在于：当一次数据修改，通知客户端，客户端再次注册watch，在这个过程中，可能数据已经发生了许多次数据修改，因此，千万不要做这样的测试：”数据被修改了n次，一定会收到n次通知”来测试server是否正常工作。（我曾经就做过这样的傻事，发现Server一直工作不正常？其实不是）。即使你使用了GitHub上这个客户端也一样。</p>
<p>8.能为临时节点创建子节点吗?<br>不能。</p>
<p>9.是否可以拒绝单个IP对ZK的访问,操作<br>ZK本身不提供这样的功能，它仅仅提供了对单个IP的连接数的限制。你可以通过修改iptables来实现对单个ip的限制，当然，你也可以通过这样的方式来解决。<a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1320" target="_blank" rel="external">https://issues.apache.org/jira/browse/ZOOKEEPER-1320</a></p>
<p>10.在getChildren(String path, boolean watch)这个API中，是注册了对节点子节点的变化，那么子节点的子节点变化能通知吗<br>不能</p>
<p>11.创建的临时节点什么时候会被删除，是连接一断就删除吗？延时是多少？<br>连接断了之后，ZK不会马上移除临时数据，只有当SESSIONEXPIRED之后，才会把这个会话建立的临时数据移除。因此，用户需要谨慎设置Session_TimeOut</p>
<p>12.zookeeper是否支持动态进行机器扩容？如果目前不支持，那么要如何扩容呢？<br>截止2012-03-15，3.4.3版本的zookeeper，还不支持这个功能，在3.5.0版本开始，支持动态加机器了，期待下吧: <a href="https://issues.apache.org/jira/browse/ZOOKEEPER-107" target="_blank" rel="external">https://issues.apache.org/jira/browse/ZOOKEEPER-107</a><br>目前只能通过修改zoo.cfg配置文件，然后逐台重启机器来实现扩容</p>
<p>13.echo stat | nc localhost $port<br>This ZooKeeper instance is not currently serving requests<br>这是最常见的问题了，一般可能有以下原因。<br>1、数据正在同步，需要等几分钟再试<br>2、可用节点数少于(n/2 + 1)，剩余的节点也无法服务（比如三个节点的zk集群，剩下一个节点，就无法服务了）<br>3、myid的数据错误了，需要zoo.cfg中的server.1(server.2,server.3)保持一致<br>4、java的通信端口不对，检查下zoo.cfg中serverlist中的端口是否每个机器一致，单机多部署时，查看是否和其他集群冲突</p>
<p>14.启动失败，进程退出<br>1、配置的log目录不存在，或者权限不够</p>
<h3 id="zookeeper动态更换机器-更换zk节点"><a href="#zookeeper动态更换机器-更换zk节点" class="headerlink" title="zookeeper动态更换机器 更换zk节点"></a>zookeeper动态更换机器 更换zk节点</h3><p>假设：目前有ip1，ip2，ip3机器，ip3坏掉了，需要更换为ip4<br>1、在ip4安装zk，同时配置文件和ip1完全一致（如果你拷贝的ip1的整个目录，记得清理数据目录）<br>2、修改ip4上myid文件，id号和坏掉的机器一致<br>3、修改ip4上zoo.cfg文件，把server.3修改为ip4<br>4、启动ip4的zk服务，这个时候ip4上不会同步任何数据<br>5、修改ip1的zoo.cfg文件，把server.3修改为ip4，重启。重启后，你仍然看不到ip4上有任何数据同步过来。<br>6、修改ip2的zoo.cfg文件，把server.3修改为ip4，重启。重启后，你现在可以看到data目录和log都有数据同步过来，一般同步几分钟就完成了，使用echo stat | nc localhost $port可以检查下。</p>
<h3 id="其它资料"><a href="#其它资料" class="headerlink" title="其它资料"></a>其它资料</h3><p><a href="https://my.oschina.net/pingpangkuangmo/blog/799041" target="_blank" rel="external">https://my.oschina.net/pingpangkuangmo/blog/799041</a></p>
<p><a href="https://my.oschina.net/pingpangkuangmo/blog/799041" target="_blank" rel="external">https://my.oschina.net/pingpangkuangmo/blog/799041</a></p>
<p><a href="http://yuzhouwan.com/posts/31915/" target="_blank" rel="external">http://yuzhouwan.com/posts/31915/</a></p>
<p><a href="http://www.aboutyun.com/forum-149-1.html" target="_blank" rel="external">http://www.aboutyun.com/forum-149-1.html</a></p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/02/14/LXC命令列表/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption"><</strong>
      <div class="article-nav-title">
        
          LXC命令列表
        
      </div>
    </a>
  
  
    <a href="/2015/06/01/Linux中mmap技术/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Linux中mmap技术</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
  
	   <! -- 添加捐赠图标 -->
<div class ="post-donate">
    <div id="donate_board" class="donate_bar center">
        <a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏"></a>
        <span class="donate_txt">
           &uarr;<br>
		   欣赏此文？求鼓励，求支持！
        </span>
        <br>
      </div>
	<div id="donate_guide" class="donate_bar center hidden" >
		<!-- 支付宝打赏图案 -->
		<img src="/img/zhifubao.jpg" alt="支付宝打赏">
		<!-- 微信打赏图案 -->
		<img src="/img/weixin.png" alt="微信打赏">
    </div>
	<script type="text/javascript">
		document.getElementById('btn_donate').onclick = function(){
			$('#donate_board').addClass('hidden');
			$('#donate_guide').removeClass('hidden');
		}
	</script>
</div>
<! -- 添加捐赠图标 -->

  
</article>


<div class="share_jia">
	<!-- JiaThis Button BEGIN -->
	<div class="jiathis_style">
		<span class="jiathis_txt">Share to: &nbsp; </span>
		<a class="jiathis_button_facebook"></a> 
    <a class="jiathis_button_twitter"></a>
    <a class="jiathis_button_plus"></a> 
    <a class="jiathis_button_tsina"></a>
		<a class="jiathis_button_cqq"></a>
		<a class="jiathis_button_douban"></a>
		<a class="jiathis_button_weixin"></a>
		<a class="jiathis_button_tumblr"></a>
    <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	</div>
	<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1405949716054953" charset="utf-8"></script>
	<!-- JiaThis Button END -->
</div>






<div class="duoshuo">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="zookeeper" data-title="zookeeper" data-url="http://thinkhejie.github.io/2015/06/02/zookeeper/"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"true"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>




</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 thinkhejie
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>